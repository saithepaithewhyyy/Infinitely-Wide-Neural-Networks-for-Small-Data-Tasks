# Infinitely Wide Neural Networks for Small Data Tasks


## Abstract 

Theoretical research has shown that an infinitely wide neural network trained under L2 loss by
gradient descent with an infinitesimally small learning rate is equivalent to kernel regression with
respect to a so called Neural Tangent Kernel (NTK). Since Kernel Methods are highly efficient
on small data tasks, we attempt to test the Neural Tangent Kernel on the UCI dataset collection,
since they have a large collection of small-data data sets and compare their performance to other
tried and tested models such as Random Forests, Support Vector Machines (SVM), and (Deep)
Neural Networks
