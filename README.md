# Infinitely Wide Neural Networks for Small Data Tasks


## Abstract 

Theoretical research has shown that an infinitely wide neural network trained under L2 loss by
gradient descent with an infinitesimally small learning rate is equivalent to kernel regression with
respect to a so called Neural Tangent Kernel (NTK). Since Kernel Methods are highly efficient
on small data tasks, we attempt to test the Neural Tangent Kernel on the UCI dataset collection,
since they have a large collection of small-data data sets and compare their performance to other
tried and tested models such as Random Forests, Support Vector Machines (SVM), and (Deep)
Neural Networks

The code and the research in the repository was done as a part of the SURGE program during the summer of 2023 and is based on the paper - Arora, S., Du, S. S., Li, Z., Salakhutdinov, R., Wang, R., & Yu, D. (2019). Harnessing the power of infinitely wide deep nets on small-data tasks. arXiv preprint arXiv:1910.01663.
